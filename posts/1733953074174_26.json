{
  "title": "TensorFlow.js로 AI 에이전트 구축하기",
  "date": 1733953074281,
  "data": "{\n  \"time\": 1734509747615,\n  \"blocks\": [\n    {\n      \"id\": \"_zicky1Ro7\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1 TensorFlow.js 기본 사용법\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"9Lc8cE1Vnv\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"이 절에서는 TensorFlow.js를 처음 사용하는 독자를 위해 기본 개념과 사용법을 소개합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"oOngcl7VPJ\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1.1. TensorFlow.js란 무엇인가?\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"7aXsfgO_5L\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"TensorFlow.js는 브라우저와 Node.js 환경에서 딥러닝을 수행할 수 있는 JavaScript 라이브러리입니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"GPU를 활용한 고속 계산이 가능하며, 기존 TensorFlow 모델을 로드하거나 새 모델을 정의할 수 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"zvgjWNHmbj\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1.2. TensorFlow.js 설치 및 초기화\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"l7IX4JTjwE\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js를 설치하고 사용하는 기본 절차를 설명합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"z7Kuckkujj\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"# 설치 명령\\nnpm install @tensorflow/tfjs\",\n        \"language\": \"bash\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"DNkAnlbzfD\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1.3. Tensor 자료구조\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"Q3yYIB8THn\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js의 핵심 자료구조인 Tensor를 소개합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"mhxrl7FbLL\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"import * as tf from '@tensorflow/tfjs';\\n\\n// 1D Tensor (벡터)\\nconst tensor1d = tf.tensor1d([1, 2, 3, 4]);\\n\\n// 2D Tensor (행렬)\\nconst tensor2d = tf.tensor2d([[1, 2], [3, 4]]);\\nconsole.log(tensor2d.toString());\\n\\n// 연산 예제: 행렬 곱\\nconst result = tf.matMul(tensor2d, tensor2d);\\nresult.print();\",\n        \"language\": \"js\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"jqsMCLCZ8o\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"텐서(Tensor):\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"XsCHO8QIe_\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"다차원 배열 구조로 데이터를 표현합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"nLn-IxD5so\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const a = tf.tensor([1, 2, 3, 4], [2, 2]); // 2x2 행렬\\nconst b = tf.scalar(5); // 스칼라 값\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"hzp2_Rywgx\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1.4. 기본적인 모델 생성\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"jocLPGAO5j\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"단순 선형 회귀 모델을 생성하고 학습하는 과정을 설명합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"86QGlnwoSh\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js에서는 Sequential 또는 Functional API로 모델을 정의합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"PmqDlIvMS9\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const model = tf.sequential();\\nmodel.add(tf.layers.dense({ units: 64, activation: 'relu', inputShape: [2] }));\\nmodel.add(tf.layers.dense({ units: 4, activation: 'softmax' }));\\nmodel.compile({ optimizer: 'adam', loss: 'categoricalCrossentropy' });\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"d_96XcKWiE\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"연산(Operations):\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"72H6FwdC6u\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"기본적인 텐서 연산은 Python TensorFlow와 유사합니다\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"F5XfbJdkSo\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const c = tf.add(a, b); // 텐서 덧셈\\nconst d = tf.matMul(a, a); // 행렬 곱\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"tc5yFcvreI\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"간단한 예제: y = 2x + 1 학습\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"eWvr9-P3vC\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"아래 코드는 TensorFlow.js로 간단한 선형 회귀 모델을 학습하는 예제입니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"VszPtman6p\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"import * as tf from '@tensorflow/tfjs';\\n\\n// 데이터 정의\\nconst xs = tf.tensor2d([0, 1, 2, 3, 4], [5, 1]); // 입력 데이터\\nconst ys = tf.tensor2d([1, 3, 5, 7, 9], [5, 1]); // 출력 데이터\\n\\n// 모델 정의\\nconst model = tf.sequential();\\nmodel.add(tf.layers.dense({ units: 1, inputShape: [1] }));\\nmodel.compile({ optimizer: 'sgd', loss: 'meanSquaredError' });\\n\\n// 모델 학습\\nasync function trainModel() {\\n  await model.fit(xs, ys, { epochs: 500 });\\n  console.log('Training complete');\\n  const prediction = model.predict(tf.tensor2d([5], [1, 1]));\\n  prediction.print(); // 예상 결과: y = 2*5 + 1 = 11\\n}\\n\\ntrainModel();\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"slSzX1YLpB\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2 심층 신경망 설계 및 구성\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"h0G73kwR_n\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"여기서는 심층 신경망(DNN)의 구조와 TensorFlow.js로 구현하는 방법을 다룹니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"kUVFpc54St\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2.1. DNN의 기본 개념\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"34FlNy9ywp\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"입력층(Input Layer): 데이터를 입력받는 첫 번째 층.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"은닉층(Hidden Layers): 입력 데이터를 처리하는 여러 계층.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"출력층(Output Layer): 최종 결과를 출력.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"kpVQRsgPtG\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2.2. 심층 신경망 설계\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"ZADb9Dm_ty\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"에이전트가 상태를 입력으로 받고 행동을 출력하도록 설계합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"VpjDpvxv5t\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const model = tf.sequential();\\n\\n// 입력층 및 첫 번째 은닉층\\nmodel.add(tf.layers.dense({ units: 128, activation: 'relu', inputShape: [stateSize] }));\\n\\n// 추가 은닉층\\nmodel.add(tf.layers.dense({ units: 64, activation: 'relu' }));\\n\\n// 출력층: 행동의 수\\nmodel.add(tf.layers.dense({ units: actionSize, activation: 'softmax' }));\\n\\n// 모델 컴파일\\nmodel.compile({\\n  optimizer: tf.train.adam(),\\n  loss: 'categoricalCrossentropy',\\n});\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"hdjmJ82Eng\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2.3. 모델 시각화 및 이해\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"N-gFMv67e6\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"각 층의 입력과 출력 크기를 설명합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"브라우저에서 TensorFlow.js 모델 구조를 확인하는 방법을 안내합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"HxLlCaVqGI\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3 Q-Learning 알고리즘 적용하기\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"B7SdHxwJcF\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"Q-Learning은 강화학습에서 사용하는 핵심 알고리즘으로, 이 절에서는 이를 TensorFlow.js로 구현하는 방법을 다룹니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"UAzqSGeSk3\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3.1. Q-Learning의 기본 개념\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"dOY19KHp_v\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"Q-값: 특정 상태에서 특정 행동을 선택했을 때 기대되는 보상.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"벨만 방정식:\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"Q(s,a)=R(s,a)+γ max​Q(s′,a)\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"여기서 R(s,a)는 현재 보상, γ 할인율.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"87C3pWM801\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3.2. Q-Learning 구현\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"6BErMS0NSW\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js로 Q-Learning의 신경망을 구현합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"SS_iAiZ_3D\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"// Q-Network 생성\\nconst qNetwork = tf.sequential();\\nqNetwork.add(tf.layers.dense({ units: 64, activation: 'relu', inputShape: [stateSize] }));\\nqNetwork.add(tf.layers.dense({ units: 64, activation: 'relu' }));\\nqNetwork.add(tf.layers.dense({ units: actionSize }));\\n\\n// 행동 선택 함수\\nfunction selectAction(state, epsilon) {\\n  if (Math.random() < epsilon) {\\n    return Math.floor(Math.random() * actionSize); // 랜덤 행동\\n  } else {\\n    return tf.tidy(() => {\\n      const qValues = qNetwork.predict(tf.tensor2d([state]));\\n      return qValues.argMax(1).dataSync()[0]; // 가장 높은 Q값 행동\\n    });\\n  }\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"85LMklFhOx\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"4 모델 훈련과 평가\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"DFfuYeHYTL\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"훈련 데이터로 모델을 학습시키고, 평가를 통해 성능을 검증합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"UVqWHSEDGz\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"4.1. 학습 알고리즘\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"MoMtTU1M99\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"상태, 행동, 보상, 다음 상태로 구성된 경험을 학습합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"Q-값을 업데이트합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"HFJPq6_Jh9\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"async function trainStep(state, action, reward, nextState, done, gamma) {\\n  const targetQ = tf.tidy(() => {\\n    const qNext = qNetwork.predict(tf.tensor2d([nextState]));\\n    const maxQNext = qNext.max(1).dataSync()[0];\\n    const target = done ? reward : reward + gamma * maxQNext;\\n\\n    const qValues = qNetwork.predict(tf.tensor2d([state]));\\n    const qUpdate = qValues.dataSync();\\n    qUpdate[action] = target;\\n\\n    return tf.tensor2d([qUpdate]);\\n  });\\n\\n  await qNetwork.fit(tf.tensor2d([state]), targetQ, { epochs: 1 });\\n  targetQ.dispose();\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"ETReB4gqu9\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"이 함수는 Q-러닝 알고리즘의 핵심인 Q-value 업데이트를 수행하는 함수입니다.강화학습에서 Q-러닝은 에이전트가 상태(state)에서 행동(action)을 선택하고 보상(reward)을 받는 과정을 통해 Q-value를 갱신하여 더 나은 정책(policy)을 학습하는 데 사용됩니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"gubSazfUQM\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"아래는 코드의 주요 부분을 단계별로 설명합니다:\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"ryjChPbrMK\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1. 매개변수\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"Iuxsk9TSXq\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"async function trainStep(state, action, reward, nextState, done, gamma) {\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"fIRZA8oek1\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"state: 현재 상태. 상태 공간은 보통 에이전트의 현재 상황을 나타냅니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"action: 현재 상태에서 에이전트가 선택한 행동(0, 1, 2, 3 등).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"reward: 행동 후 얻은 보상 값.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"nextState: 행동 후 에이전트가 도달한 다음 상태.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"done: 에피소드가 종료되었는지 여부를 나타내는 불리언 값.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"gamma: 할인율(Discount Factor). 미래의 보상에 대한 현재 가치의 중요도를 조정하는 매개변수\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"QNfNnf_YnG\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2. 다음 상태에서의 최대 Q-value 계산\",\n        \"level\": 2\n      }\n    },\n    {\n      \"id\": \"S_O3bm6p8Y\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const qNext = qNetwork.predict(tf.tensor2d([nextState]));\\nconst maxQNext = qNext.max(1).dataSync()[0];\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"jszKOtbzyx\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"qNetwork.predict\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"qNetwork는 Q-value를 예측하는 신경망입니다.\",\n                \"meta\": {},\n                \"items\": []\n              },\n              {\n                \"content\": \"여기서 nextState를 입력으로 사용해 다음 상태에서 가능한 모든 행동의 Q-value를 예측합니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          },\n          {\n            \"content\": \"qNext.max(1)\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"nextState에서 가능한 행동 중 가장 큰 Q-value를 추출합니다.\",\n                \"meta\": {},\n                \"items\": []\n              },\n              {\n                \"content\": \"이는 다음 상태에서 가장 가치 있는 행동을 나타냅니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          },\n          {\n            \"content\": \"dataSync()[0]\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"TensorFlow.js 텐서를 자바스크립트 배열로 변환한 뒤, 첫 번째 값(최대 Q-value)을 가져옵니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"Af3pXgRkjC\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3. 타겟 Q-value 계산\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"pq0411bnPt\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const target = done ? reward : reward + gamma * maxQNext;\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"Gu-cnI2_I4\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"done\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"에피소드가 종료된 경우:\",\n                \"meta\": {},\n                \"items\": [\n                  {\n                    \"content\": \"target = reward (현재 보상만 사용).\",\n                    \"meta\": {},\n                    \"items\": []\n                  }\n                ]\n              }\n            ]\n          },\n          {\n            \"content\": \"!done\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"에피소드가 종료되지 않은 경우:\",\n                \"meta\": {},\n                \"items\": [\n                  {\n                    \"content\": \"target = reward + gamma * maxQNext\",\n                    \"meta\": {},\n                    \"items\": []\n                  },\n                  {\n                    \"content\": \"현재 보상(reward)과 다음 상태에서의 최대 Q-value(maxQNext)를 할인율(gamma)로 가중하여 타겟 값을 계산합니다.\",\n                    \"meta\": {},\n                    \"items\": []\n                  }\n                ]\n              }\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"oyMVvA4YJP\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"4. 현재 상태에서 Q-value를 가져오기\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"GJg7FB6EvL\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const qValues = qNetwork.predict(tf.tensor2d([state]));\\nconst qUpdate = qValues.dataSync();\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"ZhN-jh9vDd\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"<b>qNetwork.predict</b>\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"현재 상태(state)를 입력으로 하여 모든 가능한 행동에 대한 Q-value를 예측합니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          },\n          {\n            \"content\": \"<b>qUpdate</b>\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"예측된 Q-value를 배열로 변환합니다. 이는 현재 상태에서 각 행동의 Q-value를 포함합니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"dOJGOyTwAF\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"5. 선택한 행동(action)의 Q-value 업데이트\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"kaQGeLZAI4\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"qUpdate[action] = target;\\n\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"UB5-uXKJ0k\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"<b>action</b>에 해당하는 Q-value를 위에서 계산한 타겟 Q-value로 업데이트합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"Bg5BCGYJlR\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"6. 업데이트된 Q-value로 신경망 학습\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"EeXQoPcFzh\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"await qNetwork.fit(tf.tensor2d([state]), tf.tensor2d([qUpdate]), { epochs: 1 });\\n\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"VdU9ZCI8rO\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"fit\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"업데이트된 Q-value(qUpdate)를 타겟 값으로 사용하여 신경망을 한 번의 에포크(학습 반복) 동안 학습시킵니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          },\n          {\n            \"content\": \"입력(state)과 출력(qUpdate) 쌍으로 Q-value를 학습합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"5abVlkkQ4W\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"7. 메모리 정리\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"mUtyjLIW1g\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"targetQ.dispose();\\n\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"GAflJFMWP8\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"TensorFlow.js는 메모리를 효율적으로 관리하기 위해 텐서를 명시적으로 제거해야 합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"이 부분은 GPU 메모리 누수를 방지하기 위한 코드입니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"vfuQdxLWI4\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"전체 과정 요약\",\n        \"level\": 6\n      }\n    },\n    {\n      \"id\": \"dxAX9-pYvf\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"다음 상태에서 최대 Q-value(maxQNext)를 계산합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"현재 상태에서 선택한 행동의 타겟 Q-value(target)를 계산합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"현재 상태에서 모든 Q-value를 가져옵니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"선택한 행동의 Q-value를 타겟 값으로 업데이트합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"업데이트된 Q-value를 학습하여 Q-network를 갱신합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"Wrp2eWJWE-\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"Q-러닝 공식\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"pVw1QhFP2D\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"위 코드는 Q-러닝의 핵심 공식에 기반합니다:\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"W6FkEBh9jJ\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"Q(s,a)←(1−α)Q(s,a)+α(r+γa′max​Q(s′,a′))\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"ol3bKtIuLg\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"여기서:\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"Hagu-6vExX\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"<span><span>Q(s,a)</span></span>: 현재 상태 <span><span>s</span></span>에서 행동 <span><span>a</span></span>의 Q-value.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"<span><span>α</span></span>: 학습률 (코드에서는 fit으로 학습률 조정).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"<span><span>r</span></span>: 보상.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"<span><span>γ</span></span>: 할인율.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"<span><span><span><span><span>max</span><span><span><span><span><span><span></span><span><span><span><span>a</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>Q</span><span>(</span><span><span>s</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>,</span><span></span><span><span>a</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span></span></span></span>: 다음 상태에서의 최대 Q-value.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"Rsrx7UnxpU\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"이 코드는 타겟 Q-value를 직접 계산하여 신경망에 학습시키는 방식으로 구현되어 있습니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"5jbnlENzyf\",\n      \"type\": \"delimiter\",\n      \"data\": {\n        \"style\": \"star\"\n      }\n    },\n    {\n      \"id\": \"MCpmegdV5Q\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"4.2. 평가\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"rkLRT3jQEr\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"모델의 성능을 평가하는 메트릭을 설정합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"GSPTuYxoxP\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"에이전트가 도달한 평균 보상.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"학습 후 성공률.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"98KJZaYxWu\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"4.3. 성능 시각화\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"BdVtRSjorx\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"학습 과정 중 보상 변화를 그래프로 표시합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"tf-vis 라이브러리를 사용하여 훈련 및 평가 데이터를 시각화합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"xRM4uj6x5R\",\n      \"type\": \"embed\",\n      \"data\": {\n        \"service\": \"codepen\",\n        \"source\": \"https://codepen.io/ghostwebservice/pen/raBMaoJ\",\n        \"embed\": \"https://codepen.io/ghostwebservice/embed/raBMaoJ?height=300&theme-id=0&default-tab=css,result&embed-version=2\",\n        \"width\": 600,\n        \"height\": 300,\n        \"caption\": \"training은 사용자 환경에 따라 속도가 달라질 수 있습니다.\"\n      }\n    }\n  ],\n  \"version\": \"2.30.7\"\n}",
  "id": "1733953074174_26"
}