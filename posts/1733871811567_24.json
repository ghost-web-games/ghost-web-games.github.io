{
  "title": "기본 개념 이해하기",
  "date": 1733871811321,
  "data": "{\n  \"time\": 1733871914202,\n  \"blocks\": [\n    {\n      \"id\": \"ZopOgiSJc5\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1 강화학습의 기초\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"D9jCNIEbAC\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"강화학습이란?\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"yAppc33QxN\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"강화학습(Reinforcement Learning, RL)은 에이전트(Agent)가 환경(Environment)과 상호작용하며 최적의 행동(Policy)을 학습하는 과정입니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"uqL7ppXLZP\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"이 학습 과정에서 에이전트는 <b><u class=\\\"cdx-underline\\\">보상(Reward)</u></b>을 통해 자신의 행동이 얼마나 효과적이었는지를 판단하며, 이를 반복적으로 개선합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"Bzc9z87HuH\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"강화학습의 주요 특징\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"Yz1b2QHM5a\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"에이전트는 환경과의 상호작용을 통해 학습합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"보상은 정답 대신 행동에 대한 피드백으로 제공됩니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"목표는 누적 보상(Cumulative Reward)을 최대화하는 행동을 학습하는 것입니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"e-XmMJBZt4\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"예제\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"HdEZ3KYG_H\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"문서의 강아지 게임에서:\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"74WVIwaZhB\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"에이전트: 강아지\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"환경: 게임 맵\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"행동: 위/아래/왼쪽/오른쪽으로 이동\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"보상: 먹이를 먹으면 +10, 출구에 도달하면 +50, 적에게 부딪히면 -20\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"XLpn9EXhL_\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"강화학습의 용어 정리\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"N1FTsm9gob\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"상태(State): 현재 에이전트가 처한 환경의 정보 (예: 강아지의 위치).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"행동(Action): 에이전트가 취할 수 있는 움직임 또는 선택.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"보상(Reward): 행동의 결과로 주어지는 값.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"정책(Policy): 상태에서 행동을 결정하는 규칙.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"에피소드(Episode): 시작 상태에서 종료 상태까지의 일련의 행동 과정.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"eecn3pEhFU\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2 강화학습의 주요 요소\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"SbvUejCpP6\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"1. 상태(State)\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"qiatfmeAkM\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"환경에서 에이전트가 관찰할 수 있는 정보입니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"me1zm5UFGn\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"예제: 게임 맵의 좌표와 강아지의 위치.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"구현: 2D 배열 또는 JSON 구조로 상태를 표현할 수 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"HGIMJA_mfk\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"2. 행동(Action)\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"i8-Mridlc0\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"에이전트가 상태에 따라 취할 수 있는 움직임 또는 결정합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"kWHXsDZWBI\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"예제: 강아지가 이동 가능한 4방향 (위, 아래, 왼쪽, 오른쪽).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"구현: 정수로 행동을 매핑 (0: 위, 1: 아래, 2: 왼쪽, 3: 오른쪽).\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"RlKRfYLdNF\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"3. 보상(Reward)\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"camCfeDeCX\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"행동이 얼마나 유익했는지 측정하는 값입니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"qzanMsQ-Pv\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"예제:출구에 도달: +50먹이 획득: +10적과 충돌: -20\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"구현: 행동에 따라 점수를 반환하는 함수 작성.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"5FqW6nWb6e\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"4. 에이전트(Agent)\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"DAovlEcwA5\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"환경과 상호작용하여 최적의 정책을 학습하는 주체입니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"vSeIHnGRm_\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"예제: 강화학습 알고리즘으로 동작하는 강아지.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"구현: TensorFlow.js 모델로 학습하는 신경망.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"aRDXEnf_RI\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"5. 환경(Environment)\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"L1tV4Z4aiq\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"에이전트가 동작하는 세계로, 상태와 보상을 정의합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"FTA6fRpwZ7\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"예제: Three.js로 구성된 게임 맵.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"구현: 상태와 보상을 반환하는 함수 작성.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"EoNqSPa6ab\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3 TensorFlow.js로 강화학습하기\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"hbE_on9ll3\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js란?TensorFlow.js는 브라우저에서 머신러닝 모델을 훈련하고 실행할 수 있는 JavaScript 라이브러리입니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"9X8ZczbrkV\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js의 주요 기능\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"1wyOCSBDqW\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"모델 훈련: 신경망을 정의하고 학습 가능.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"모델 실행: 사전 훈련된 모델을 로드해 예측 수행.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"GPU 가속: 브라우저의 WebGL을 활용해 연산 속도를 높임.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"BR6YhEHHsM\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"강화학습에서의 활용\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"oEfSnvbaHC\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"상태를 입력으로 받아 행동(Action)을 예측하는 모델 구축.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"예제: Q-Learning 알고리즘을 신경망으로 구현.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"ThED1pKa95\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"Q-Learning 개요\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"1XZXk-BDXB\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"Q-Learning은 강화학습의 기초적인 알고리즘으로, 각 상태-행동 쌍의 가치를 학습합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"Q 함수: 상태와 행동의 쌍에 대해 누적 보상을 추정하는 함수.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"Q(s,a)=R+γ⋅maxQ(s′,a′)\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"<span><span><span><span>s</span></span></span></span>: 현재 상태\",\n                \"meta\": {},\n                \"items\": []\n              },\n              {\n                \"content\": \"<span><span><span><span>a</span></span></span></span>: 현재 행동\",\n                \"meta\": {},\n                \"items\": []\n              },\n              {\n                \"content\": \"<span><span><span><span>R</span></span></span></span>: 보상\",\n                \"meta\": {},\n                \"items\": []\n              },\n              {\n                \"content\": \"<span><span>γ</span></span>: 미래 보상을 고려하는 할인율\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"RSUxkZdLUL\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js로 Q-Learning 신경망 설계하기\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"ObZw87Qllj\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const model = tf.sequential();\\nmodel.add(tf.layers.dense({ units: 64, activation: 'relu', inputShape: [stateSize] }));\\nmodel.add(tf.layers.dense({ units: 64, activation: 'relu' }));\\nmodel.add(tf.layers.dense({ units: actionSize, activation: 'linear' }));\\nmodel.compile({ optimizer: 'adam', loss: 'meanSquaredError' });\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"COQLwTPhIj\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"stateSize: 상태의 차원(예: 강아지의 위치).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"actionSize: 가능한 행동의 수(4방향).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"relu: 비선형 활성화 함수로 신경망 학습 가속화.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"linear: 행동 가치(Q 값)를 출력.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"i08DdtG3OP\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"훈련 데이터 생성Three.js에서 에이전트가 움직인 결과(상태, 행동, 보상)를 기록해 TensorFlow.js로 훈련합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"s8AKuuAb3O\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"4 사례를 통해 개념 정리하기\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"WZiqB71vPH\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"강아지가 출구를 찾는 강화학습 예제\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"I1eVtbwYNk\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"상태: 강아지의 현재 위치.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"행동: 위/아래/왼쪽/오른쪽 이동.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"보상:출구 도달 시 +50먹이를 먹으면 +10적과 충돌하면 -20\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"Bviwfmd4bI\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"TensorFlow.js로 학습한 Q 값 결과 예측\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"MMOtZS6kYB\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"입력: 강아지의 현재 위치 상태.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"출력: 각 행동의 Q 값 (가치).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"강아지는 가장 높은 Q 값을 가지는 행동을 선택합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"zfmqZU_0pC\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"이 장에서는 강화학습과 TensorFlow.js를 활용한 기본 개념을 명확히 정리하고, 이후의 실습에 필요한 기초 이론을 제공합니다.다음 장에서는 이 개념을 바탕으로 Three.js를 활용해 3D 게임 환경을 구성하는 방법을 다룹니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    }\n  ],\n  \"version\": \"2.30.7\"\n}",
  "id": "1733871811567_24"
}