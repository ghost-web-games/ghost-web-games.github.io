{
  "title": "강화학습 개선하기",
  "date": 1734140439537,
  "data": "{\n  \"time\": 1734141415464,\n  \"blocks\": [\n    {\n      \"id\": \"fBcjld3chb\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"이전 장에서 작성된 강화학습 코드를 실행하면 학습이 잘 안된다는 것을 알 수 있습니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"LFvWYpomHM\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"학습률이 올라가지 않는 이유는 여러 가지가 있을 수 있습니다. Q-learning 알고리즘에서 학습이 제대로 이루어지지 않는 원인은 대부분 Q-값의 업데이트가 제대로 이루어지지 않거나, 보상 신호가 충분히 강하지 않거나, 탐험 비율(epsilon)이 너무 급격하게 감소하는 경우입니다. 아래에서 몇 가지 점검해야 할 사항과 해결 방법을 제시하겠습니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"n_fHLi3ecf\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1. 학습률(learning rate)과 Q-값 업데이트\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"1qbKzgYQtT\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"현재 코드에서 qNetwork.fit()을 통해 Q-값을 업데이트하고 있습니다. 그러나 학습이 제대로 이루어지지 않는 이유는 다음과 같을 수 있습니다:\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"e3r_bSecpn\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"작은 보상: reward가 항상 -0.1이고, 학습 시 탐색 행동이 너무 많다면 Q-값의 변화가 미미할 수 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"Q-값 업데이트: trainStep 함수 내에서 qUpdate를 업데이트한 뒤, 이를 qNetwork.fit에 전달하고 있습니다. 이 부분은 계산된 타겟 Q-값과 예측된 Q-값 사이의 차이를 줄이기 위해 더 강한 신호가 필요할 수 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"JjYVI5Bd1J\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"해결 방법:\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"F0VSlNHIGj\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"Q-값 업데이트 강화를 위해 targetQ를 계산할 때, gamma * maxQNext를 줄이는 대신 더 높은 값을 유지하거나, reward와 더 많이 결합해보세요.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"타겟 Q값 계산을 할 때 더 길게 학습되도록 수정하거나, 다양한 탐색을 통해 보상을 제대로 반영하도록 합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"zuOByUO3qy\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2. 탐험 비율 epsilon의 감소\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"9gKE24bRX9\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"탐험 비율(epsilon)이 너무 빨리 감소하면, 초반에 학습할 수 있는 충분한 양의 랜덤 탐색이 이루어지지 않습니다. 초기에 너무 낮은 epsilon 값은 에이전트가 랜덤하게 행동할 기회를 줄여서 학습이 느려질 수 있습니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"TfvD6q6yyu\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"해결 방법:\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"QqLanlU_Ix\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"epsilonDecay 값을 더 천천히 감소시키거나, epsilon이 너무 빨리 0에 가까워지지 않도록 조절합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"iA3uUiBcpm\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"let epsilon = 1.0; // 초기 값\\nconst epsilonDecay = 0.999; // 감소율을 천천히 낮추기\\nconst epsilonMin = 0.1; // 최소 epsilon 값 설정\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"YTnL81X1Ud\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3. 보상 체계 검토\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"nN80u5DvwA\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"맞습니다. 에이전트가 목표(출구)에 점진적으로 접근하도록 하기 위해, 거리 기반 보상 시스템을 도입하면 학습 효율을 크게 개선할 수 있습니다. 목표와의 거리가 가까워질수록 보상을 주고, 멀어지면 페널티를 주면 에이전트는 올바른 방향으로 이동하도록 유도될 가능성이 높아집니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"ThgnAr2GI8\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"거리 기반 보상 설계\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"Qikq2_ZIwg\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"현재 거리 계산에이전트의 위치와 목표 위치 간의 유클리드 거리를 계산합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"보상 정의이전 거리(prevDistance)보다 현재 거리(currentDistance)가 작아졌다면 양의 보상을 줍니다.반대로 거리가 멀어졌다면 음의 페널티를 줍니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"종료 조건목표 위치에 도달했을 때, 큰 보상을 주는 기존 로직은 유지합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"QkRx9-m5hU\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"코드 수정\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"bUjNOkREd6\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"function getReward() {\\n  const currentDistance = Math.sqrt(\\n    Math.pow(agentPosition.x - exitPosition.x, 2) +\\n    Math.pow(agentPosition.y - exitPosition.y, 2)\\n  );\\n\\n  // 목표에 도달했을 때 보상\\n  if (agentPosition.x === exitPosition.x && agentPosition.y === exitPosition.y) {\\n    return 10; // 높은 보상\\n  }\\n\\n  // 거리 기반 보상: 가까워지면 +1, 멀어지면 -1\\n  const reward = - currentDistance;\\n  return reward; \\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"86sIbEIkWI\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"보상의 체계 검토\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"Se3mNnfeHE\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"코드에서 Reward가 0에 수렴하도록 보이는 이유는 학습 목표가 \\\"높은 보상을 얻는 것\\\"이 아니라 보상 신호가 학습에 적절히 반영되지 못한 설계 문제 때문일 가능성이 큽니다. 이를 해결하기 위해 보상 설계와 Q-Learning의 목표 업데이트 로직을 점검해야 합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"NjAIABCoA9\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"문제 원인 분석\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"oyjrBORya-\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"보상이 부족한 정보 제공: 이것은 학습에 매우 큰 영향을 줍니다.\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"현재 보상 구조:\",\n                \"meta\": {},\n                \"items\": [\n                  {\n                    \"content\": \"출구에 도달하면 보상 = 10\",\n                    \"meta\": {},\n                    \"items\": []\n                  },\n                  {\n                    \"content\": \"이동 시 보상 = -0.1\",\n                    \"meta\": {},\n                    \"items\": []\n                  }\n                ]\n              },\n              {\n                \"content\": \"문제점:\",\n                \"meta\": {},\n                \"items\": [\n                  {\n                    \"content\": \"출구에 도달하지 않는 한 대부분의 행동에서 얻는 보상이 비슷합니다(-0.1). \",\n                    \"meta\": {},\n                    \"items\": []\n                  },\n                  {\n                    \"content\": \"에이전트가 출구 도달을 학습하기 위해 필요한 정보를 충분히 제공하지 못하고 있습니다.\",\n                    \"meta\": {},\n                    \"items\": []\n                  },\n                  {\n                    \"content\": \"\\\"거리를 기준으로 한 보상\\\" 등 추가적인 지표가 필요합니다.\",\n                    \"meta\": {},\n                    \"items\": []\n                  }\n                ]\n              },\n              {\n                \"content\": \"패널티가 작게 설정되어 있는 항목이 있다면 목표를 이루기 보다 그 방향으로 수렴하는 잘못된 결과가 나올 수 있습니다. 패널티는 선형적으로 목표를 이루는 방향으로 설정되어야합니다\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"ZEd-vqgV8Q\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"start\": 2,\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"Q-Learning의 타겟 업데이트 로직 문제Q-값 업데이트 로직이 보상 신호를 제대로 반영하지 못할 수 있습니다.현재 코드에서 Q-값 업데이트:\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"-oJWEi7iPa\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const target = done ? reward : reward + gamma * maxQNext;\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"fC0J21OstB\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"done 상태에서는 보상만 반영(reward).\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"그 외에는 미래 보상(gamma * maxQNext)를 합산.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"문제점:\",\n            \"meta\": {},\n            \"items\": [\n              {\n                \"content\": \"maxQNext가 초기화되지 않았거나, 학습 초기에 충분히 높은 Q-값이 생성되지 않아 업데이트가 느리게 진행될 수 있습니다.\",\n                \"meta\": {},\n                \"items\": []\n              }\n            ]\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"tiU4AX_vCb\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"start\": 3,\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"탐험 비율 감소 문제\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"snWKFM-ZZL\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"epsilon이 학습 초기부터 빠르게 감소(epsilon *= epsilonDecay)하여, 에이전트가 충분히 탐험하지 못하고 조기 수렴할 가능성이 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"탐험 비율 epsilon의 감소 속도를 더 완만하게 조정\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"RJA9rAVkWg\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"epsilon = Math.max(0.1, epsilon * epsilonDecay); // 최소값 설정\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"qmo9KHugGF\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"초기에 충분한 탐험이 이루어지도록 설정\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"JMIe-peWDO\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const epsilonDecay = 0.999; // 느린 감소율\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"A43dBTItss\",\n      \"type\": \"delimiter\",\n      \"data\": {\n        \"style\": \"dash\"\n      }\n    },\n    {\n      \"id\": \"NKZ0-w3cZ0\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"추가 개선 사항\",\n        \"level\": 5\n      }\n    },\n    {\n      \"id\": \"vxwgl7BoAA\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"점진적 보상 강화: 거리 차이가 작을 때 보상이 너무 작아 학습이 느려질 수 있습니다. 이를 방지하려면 거리 차이에 가중치를 부여할 수 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"8k1nph2vUd\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const weight = 5; // 보상 강화 계수\\nconst reward = (prevDistance - currentDistance) * weight;\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"VHBYYxb-qo\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"비선형 보상: 거리가 아주 가까워지면 더 높은 보상을 주고, 멀어지면 페널티를 기하급수적으로 증가시킬 수 있습니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"lUoJtCxqZJ\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"const reward = prevDistance - currentDistance;\\nreturn Math.sign(reward) * Math.pow(Math.abs(reward), 1.5); // 비선형 가중치\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"9F3YQWoa4W\",\n      \"type\": \"embed\",\n      \"data\": {\n        \"service\": \"codepen\",\n        \"source\": \"https://codepen.io/ghostwebservice/pen/raBWNeg\",\n        \"embed\": \"https://codepen.io/ghostwebservice/embed/raBWNeg?height=300&theme-id=0&default-tab=css,result&embed-version=2\",\n        \"width\": 600,\n        \"height\": 300,\n        \"caption\": \"이전 코드보다 좀 더 개선되었습니다.\"\n      }\n    }\n  ],\n  \"version\": \"2.30.7\"\n}",
  "id": "1734140439660_29"
}