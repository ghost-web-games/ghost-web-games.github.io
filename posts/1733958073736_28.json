{
  "title": "게임 환경과 강화학습 통합하기",
  "date": 1733958073627,
  "data": "{\n  \"time\": 1734818342913,\n  \"blocks\": [\n    {\n      \"id\": \"MvOpncePYj\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"1 Three.js와 TensorFlow.js 데이터 교환\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"DGf6UG_cPG\",\n      \"type\": \"image\",\n      \"data\": {\n        \"caption\": \"5x5 환경에서 초록색이 빨간색을 탐색합니다.\",\n        \"withBorder\": false,\n        \"withBackground\": false,\n        \"stretched\": false,\n        \"file\": {\n          \"url\": \"uploads/Screenshot_20241222_065815_Chrome.jpg\"\n        }\n      }\n    },\n    {\n      \"id\": \"vEmbXXaq0r\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"목표\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"REvhsbPOQY\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"Three.js의 3D 환경과 TensorFlow.js 강화학습 모델 간의 데이터를 실시간으로 교환하여 게임 속 에이전트의 학습과 행동을 동기화합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"dQP0AOvBwb\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"상세 내용\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"AwBv_ty4LS\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"1.Three.js에서 상태 추출\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"MM2HIPGR5a\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"에이전트의 위치, 주변 장애물 거리, 목표물과의 거리 등 게임 환경의 상태를 TensorFlow.js 모델의 입력 데이터로 변환합니다. \"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"PLNDB1Ab7h\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"function getGameState(agent, exitPosition, obstacles) {\\n    const agentX = agent.position.x / 10; // Normalize position\\n    const agentY = agent.position.y / 10;\\n    const distanceToExit = Math.sqrt(\\n        Math.pow(exitPosition.x - agent.position.x, 2) +\\n        Math.pow(exitPosition.y - agent.position.y, 2)\\n    ) / 10;\\n    return [agentX, agentY, distanceToExit];\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"MaGxNWwo1r\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"2. 모델 행동 전달\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"AP1Et9CIMl\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \" TensorFlow.js 모델의 예측 결과(행동)를 Three.js에서 처리하여 에이전트가 이동합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"w6hAscvlia\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"async function getActionFromModel(state, qNetwork) {\\n    const action = tf.tidy(() => {\\n        const stateTensor = tf.tensor2d([state]);\\n        const qValues = qNetwork.predict(stateTensor);\\n        return qValues.argMax(1).dataSync()[0];\\n    });\\n    return action;\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"42ed273MsH\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"3. 실시간 데이터 교환 흐름\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"F6M3gtYCfX\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"&nbsp;Three.js 게임 루프에서 에이전트 상태를 수집하고 TensorFlow.js 모델로 전달합니다.모델로부터 행동(Action)을 반환받아 Three.js 환경에 반영합니다. \"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"icd6Hd11AC\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"2 에이전트의 행동 제어 및 보상 계산\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"9qpWyHEUAX\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"목표\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"l7WiQLqLuk\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"모델이 선택한 행동을 바탕으로 에이전트를 제어하고, Three.js 환경에서 보상을 계산하여 학습 데이터로 활용합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"BCYtQxg5UT\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"상세 내용\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"g0mUydA9QN\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"ordered\",\n        \"meta\": {\n          \"counterType\": \"numeric\"\n        },\n        \"items\": [\n          {\n            \"content\": \"행동 정의\\n에이전트의 가능한 행동을 상, 하, 좌, 우로 정의합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"GfvwSRR-EG\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"function applyAction(agent, action) {\\n    switch (action) {\\n        case 0: agent.position.y += 1; break; // 상\\n        case 1: agent.position.y -= 1; break; // 하\\n        case 2: agent.position.x -= 1; break; // 좌\\n        case 3: agent.position.x += 1; break; // 우\\n    }\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"AiOlAtgNzK\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"보상 계산\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"0AkupRWeoR\",\n      \"type\": \"list\",\n      \"data\": {\n        \"style\": \"unordered\",\n        \"meta\": {},\n        \"items\": [\n          {\n            \"content\": \"목표 지점에 도달하면 높은 보상(10점)을 제공합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"일반 이동에는 작은 페널티(-0.1)를 부여하여 최적 경로 탐색을 유도합니다.\",\n            \"meta\": {},\n            \"items\": []\n          },\n          {\n            \"content\": \"장애물에 부딪히면 큰 페널티(-1점)를 부여합니다.\",\n            \"meta\": {},\n            \"items\": []\n          }\n        ]\n      }\n    },\n    {\n      \"id\": \"DINQV3WXKD\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"function calculateReward(agent, exitPosition, obstacles) {\\n    if (agent.position.x === exitPosition.x && agent.position.y === exitPosition.y) {\\n        return 10; // 목표 도달\\n    }\\n    for (let obs of obstacles) {\\n        if (agent.position.x === obs.x && agent.position.y === obs.y) {\\n            return -1; // 장애물 충돌\\n        }\\n    }\\n    return -0.1; // 일반 이동\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"xg3jXxatpY\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"상태 전환 처리 행동 후 에이전트의 상태 변화를 계산하고, 이를 학습 데이터로 TensorFlow.js 모델에 전달합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"U092liusA0\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"function updateEnvironment(agent, exitPosition, obstacles, qNetwork, done) {\\n    const state = getGameState(agent, exitPosition, obstacles);\\n    const action = await getActionFromModel(state, qNetwork);\\n    applyAction(agent, action);\\n    const reward = calculateReward(agent, exitPosition, obstacles);\\n    const nextState = getGameState(agent, exitPosition, obstacles);\\n\\n    return { state, action, reward, nextState, done: reward === 10 };\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"fh4Lb9Z-76\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"3 실시간 학습과 동작 테스트\",\n        \"level\": 3\n      }\n    },\n    {\n      \"id\": \"ZPaiUAoaBH\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"목표\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"2hgBPxy5bB\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"Three.js 환경에서 에이전트가 실시간으로 학습하여 목표를 달성하는 모습을 시뮬레이션합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"V-OESMPy1P\",\n      \"type\": \"header\",\n      \"data\": {\n        \"text\": \"상세 내용\",\n        \"level\": 4\n      }\n    },\n    {\n      \"id\": \"CIVqa-PsxZ\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"1. 학습 루프 실행\\n\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"PCL1bShnZY\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"게임 루프 내에서 TensorFlow.js 모델의 학습 데이터를 업데이트하고, 에이전트의 상태를 실시간으로 변경합니다. \"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"UIfM37nyxw\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"async function trainAgent(agent, exitPosition, obstacles, qNetwork, gamma) {\\n    for (let episode = 0; episode < 100; episode++) {\\n        agent.position.set(0, 0, 0); // 초기화\\n        let done = false;\\n\\n        while (!done) {\\n            const { state, action, reward, nextState, done: episodeDone } =\\n                await updateEnvironment(agent, exitPosition, obstacles, qNetwork, done);\\n            const targetQ = reward + (episodeDone ? 0 : gamma * Math.max(...nextState));\\n            await qNetwork.fit(tf.tensor2d([state]), tf.tensor2d([[targetQ]]), { epochs: 1 });\\n            done = episodeDone;\\n        }\\n\\n        console.log(`Episode ${episode + 1}: Completed`);\\n    }\\n}\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"m8dVsRTSK5\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"2. Three.js와 통합 \"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"YAQsZru3SF\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"Three.js의 animate 함수 내에서 모델 학습 및 에이전트의 행동을 적용합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"GCW4Ge9WJx\",\n      \"type\": \"code\",\n      \"data\": {\n        \"code\": \"function animate() {\\n    requestAnimationFrame(animate);\\n\\n    if (!trainingComplete) {\\n        trainAgent(agent, exitPosition, obstacles, qNetwork, gamma).then(() => {\\n            trainingComplete = true;\\n        });\\n    }\\n\\n    renderer.render(scene, camera);\\n}\\nanimate();\",\n        \"language\": \"javascript\",\n        \"showlinenumbers\": true\n      }\n    },\n    {\n      \"id\": \"lDfe3DCaj-\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"3. 시각화 및 로그 출력 \"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"xO2Jx43zHs\",\n      \"type\": \"paragraph\",\n      \"data\": {\n        \"text\": \"학습이 진행될수록 에이전트가 목표에 빠르게 도달하는 모습을 시각적으로 확인할 수 있도록 화면에 로그 데이터를 출력하거나 에이전트의 움직임을 강조합니다.\"\n      },\n      \"tunes\": {\n        \"textVariant\": \"\"\n      }\n    },\n    {\n      \"id\": \"n3YEdOcYf_\",\n      \"type\": \"embed\",\n      \"data\": {\n        \"service\": \"codepen\",\n        \"source\": \"https://codepen.io/ghostwebservice/pen/bNbwRvQ\",\n        \"embed\": \"https://codepen.io/ghostwebservice/embed/bNbwRvQ?height=300&theme-id=0&default-tab=css,result&embed-version=2\",\n        \"width\": 600,\n        \"height\": 300,\n        \"caption\": \"학습을 진행하면서 빠른 패스로 빨간 상자에 다가갑니다.\"\n      }\n    }\n  ],\n  \"version\": \"2.30.7\"\n}",
  "id": "1733958073736_28"
}